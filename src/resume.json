[{  "id":"0",
    "name":"om",
    "type":"user",
    "contents":["Aspiring wizard in Web Development and Data Science. I like to call myself a DataDev :D"],
    "links":"https://github.com/omegaji"
},
{  "id":"1",
    "name":"Development",
    "category":"dev",
    "type":"none"
},
{  "id":"2",
    "name":"Data Science",
    "category":"ds",
    "type":"none"
},
{"id":"3",
"name":"Vellore Institute of Technology",
"type":"education",
"cgpa":"8.82",
"yearPassout":"2021"
},
{
    "id":"4",
    "type":"project",
    "name":"projects",
    "category":"ds"
},
{
    "id":"5",
    "type":"project",
    "name":"COVID-19 Forecast",
    "category":"ds",
    "skillTag":["Python","Bokeh","Plotly","Seaborn","Pandas","Numpy","XGBoost"
    ],
    "contents":["I created a Covid 19 EDA on an old dataset provided by Kaggle. It was also used to forecast the fatality and cases.",
    "Moreover, I performed country specific EDA(India)."],
    "links":"https://www.kaggle.com/omegaji/covid-19-india-maps-eda-xgboost"
},
{
    "id":"6",
    "type":"project",
    "name":"Udemy EDA",
    "category":"ds",
    "skillTag":["Python","Pandas","Numpy","Altair","Interactive Graphs"],
    "contents":["Using the Udemy Course dataset, a lot of visualisations and data analysis has been done","I used Altair library to make interactive graphs and answered many datapoints arising from the dataset"],
    "links":"https://www.kaggle.com/omegaji/udemy-eda"
},
{

    "id":"7",
    "type":"project",
    "name":"Sentiment word extraction",
    "category":"ds",
    "skillTag":["Python","Pandas","Numpy","DistilBERT","QuestionAnswering","transformers"],
    "contents":["The kaggle competiton to extract the words which provide sentiment to the tweet was an excellent test of transformers true power.","I used Altair library to make interactive graphs and answered many datapoints arising from the dataset"],
    "links":"https://www.kaggle.com/omegaji/simple-easypeasy-distilbert-qa-d"


},
{

    "id":"8",
    "type":"project",
    "name":"Revenue Forecast",
    "category":"ds",
    "skillTag":["Python","Pandas","Numpy","Timeseries","ARIMA","Exponential Smoothing","ADF tests"],
    "contents":["Perfore time-series analysis of Quarterly revenue data of Telecom industries","Preprocessed and conducted stationary tests, as well as making the data stationary using Autocorrelation","Used ARIMA and Exponential smoothing models for final forecast"]


},

{

    "id":"9",
    "type":"project",
    "name":"Reinforcement projects",
    "category":"ds",
    "skillTag":["Genetic algorithm","MinMax Algorithm","Javascript","Processing.js","Reinforcement learning"],
    "contents":["Made unbeatable TicTacToe AI using javascript and minmax algorithm from scratch","Using p5.js and vanilla javascript created path finding rockets that learn to reach the goal by avoiding the obstacles using genetic algorithm"]


},
{

    "id":"10",
    "type":"project",
    "name":"Bear classifier",
    "category":"ds",
    "skillTag":["CNN","Python","FastAI","Opencv2","BeautifulSoup","Scraping"],
    "contents":["A classifier model for bear made using FastAI library built on Pytorch and a dataset scraped from google images","It used transfer learning on the CIFAR dataset for a quicker and accurate model."],
    "links":"https://github.com/omegaji/teddy_grizzly_black-bear-classifier"

},
{

    "id":"11",
    "type":"project",
    "name":"Rss Scraper",
    "category":"ds",
    "skillTag":["BeautifulSoup","Scraping","Python","Feedparser","Selenium"],
    "contents":["First part of the project is that it will detect if the website has Feeds or Rss enabled.","The second part is to fetch all the working RSS links."],
    "links":"https://github.com/omegaji/RssScrape"
},
{

    "id":"12",
    "type":"work",
    "name":"work experience",
    "category":"ds"
},
{

    "id":"13",
    "type":"work",
    "name":"Atyantik Technologies",
    "category":"ds"
},
{

    "id":"14",
    "type":"work",
    "name":"NLP Grader",
    "category":"ds",
    "skillTag":["Spacy","Gensim","Jaccard","Word2Vec","Doc2Vec","BERT","Python"],
    "contents":["An Automatic NLP based Grader for marksheets of students(Science Stream).","We used a combination of Rulebased and Word embedding models for a better accuracy score. Moreover, we used different similarity measures such as Cosine,Jaccard,Euclidian and Manhattan to regress the optimal score."],
    "duration":["September 2019","November 2019"]

},
{

    "id":"15",
    "type":"work",
    "name":"Amilcar Technologies",
    "category":"ds"
},
{

    "id":"16",
    "type":"work",
    "name":"AI Resume Parser",
    "category":"ds",
    "skillTag":["Python","Flask","Pandas","NLTK","Spacy","Python","Plotly","HTML","CSS","Javascript"],
    "contents":["I was tasked to create Rule based unsupervised Resume parser, to segregate the resumes into skills,work experience,name,region,education and other filters which aided the HR of the company","The second half of the project was to create a Flask frontend for the HR in which the extracted portion of resumes would be stored in the form of a table decorated with CSS","The table would contain columns such as skills,education,name,contact,work experience,experience duration and other filters, which the HR could sort them according to his or her needs."],
    "duration":["June 2020","August 2020"]

},
{

    "id":"17",
    "type":"work",
    "name":"Revenue Prediction",
    "category":"ds",
    "skillTag":["Python","SQL","Pandas","Xgboost","Regression","ScikitLearn","Seaborn","Altair"],
    "contents":["This was a small project of Revenue prediction of a customer for the year 2020.","Moreover, the SQL file was generated and extracted into CSV for further data exploration and analysis. A range of regression techniques were used and feature engineering the table was also performed."],
    "duration":["June 2020","August 2020"]

},
{

    "id":"18",
    "type":"work",
    "name":"Playstore Scraping",
    "category":"ds",
    "skillTag":["Python","BeautifulSoup","Selenium","Pandas","Classification","Regression","ScikitLearn","Altair"],
    "contents":["This was a tricky project, as Playstore scraping of more than 40k apps was performed with use of 3rd party APK sites.","Playstore shows limited apps by default, hence using 3rd party APK sites which have Apps appearing on numbered pages, I was able to create a script to match Playstore data and APK data to scrape various features of the app(about 40 columns).","Moreover, I was asked to lead a 4 member team to scrape such huge data and create a script that had many fail-safes"],
    "duration":["June 2020","August 2020"]

},

{
    "id":"19",
    "type":"work",
    "name":"Twimbit",
    "category":"ds"
}
]